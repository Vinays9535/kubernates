
                                                                                                  1). namespace object creation

* Shows all namespaces currently in the cluster:
  kubectl get namespaces

* This creates a new namespace named amma
  kubectl create namespace amma

* View namespace details ,Shows labels, annotations, resource quotas, and status of the namespace.
  kubectl describe namespace amma

                                                                                    (first create a custom name space after we set deafult namespace)

* Check current context namespace, show default name space
  kubectl config view --minify | grep namespace:

* Set new default namespace
  kubectl config set-context --current --namespace=amma

* Check current context namespace, show default name space
  kubectl config view --minify | grep namespace:

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                    2). Create secret object 
                                                                                                     
* Create secret object command
  kubectl create secret docker-registry my-docker-secret \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=vinaykumars064 \
  --docker-password=vini9535@ \
  --docker-email=vinaykumars064@gmail.com


* üîπ Explanation:

--my-docker-secret ‚Üí Secret name.

--docker-server ‚Üí Registry URL (https://index.docker.io/v1/ for Docker Hub).

--docker-username ‚Üí Your Docker Hub username.

--docker-password ‚Üí Your Docker Hub password / access token.

--docker-email ‚Üí Your email. 


*  kubectl get secrets  or  kubectl get svc
*  kubectl describe secret my-docker-secret


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

                                                                                                   
                                                                                              3). pod object creation with secret and container port

Uses a Secret for private Docker registry login

Runs a container with an exposed container port    

* first create one file cold pod.yaml
  touch pod.yaml
  vim pod.yaml
  cat pod.yaml
  

example:::

   apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: my-pod
spec:
  containers:
  - name: my-pod
    image: vinaykumars064/cicd-repo:latest                # private image
    ports:
    - containerPort: 80                            # container port exposed
  imagePullSecrets:
  - name: my-docker-secret                         # secret created earlier



  Steps to Use ::

  *  Apply the Pod
     kubectl apply -f pod.yaml

  *  Verify pod status
     kubectl get pods
     kubectl describe pod my-pod




  ‚úÖ What happens here?

      Kubernetes uses my-docker-secret to log in to Docker Hub (or your registry).

      The Pod pulls private-nginx:latest.

      The container exposes port 80 internally (you can later attach a Service to make it accessible). 

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------      

      
                                                                                          4). deployment object creation with secret and container port and replicas



Here‚Äôs a ready-to-use deployment.yaml that creates a Deployment in Kubernetes with:

*Secret for pulling a private Docker image
*Container port exposed
* with 2 replicas


* first create one file cold deployment.yaml
  touch deployment.yaml
  vim deployment.yaml
  cat deployment.yaml
  

example:::



apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
  labels:
    app: my-app
spec:
  replicas: 2                                # number of Pods to run
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app-container
        image: your-docker-user/your-private-image:latest   # replace with your private image
        ports:
        - containerPort: 8080                               # expose port inside container
      imagePullSecrets:
      - name: my-docker-secret                              # secret created earlier




1. Apply the Deployment
kubectl apply -f deployment.yaml

2. Verify
kubectl get deployments
kubectl get pods
kubectl describe deployment my-app-deployment


‚úÖ Explanation

replicas: 2 ‚Üí runs 2 Pods for scaling.

containerPort: 8080 ‚Üí container listens on port 8080.

imagePullSecrets ‚Üí allows Kubernetes to use the secret when pulling from a private Docker registry.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                 5). service object 

                                                                                                    A) cluster ip 


*create a deployment object with docker secrets and service ( cluster ip ) and port number with 2 replicas

touch deployment.ymal : create one file
vim deployment.yaml : copy and paste


apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  labels:
    app: my-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-deployment
  template:
    metadata:
      labels:
        app: my-deployment
    spec:
      containers:
      - name: my-deployment
        image: vinaykumars064/cicd-repo:latest   # replace with your private image
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: my-docker-secret

---
apiVersion: v1
kind: Service
metadata:
  name: my-service
  labels:
    app: my-service
spec:
  type: ClusterIP
  selector:
    app: my-service
  ports:
  - port: 8080        # Service port (internal)
    targetPort: 8080  # Container port


* cat deployment.yaml : describe


Apply YAML
* kubectl apply -f deployment.yaml

Verify Service
* kubectl get svc my-app-service


You should see something like:

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
my-app-service   ClusterIP   10.96.123.45   <none>        8080/TCP   10s


This means your pods are internally reachable within the cluster at http://my-app-service:8080


this deployment and service internally working or not 
using this command to login the conatainer
* kubectl exec -it <full conatainer name > -- /bin/bash




then check : 

How to correctly test?

‚úÖ From inside the cluster (a Pod):

kubectl exec -it <your-pod-name> -- curl http://172.20.85.52:8080


‚úÖ From a Service (ClusterIP):
If you already created a ClusterIP service for your deployment:

* kubectl get svc


Example output:

NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
my-app-service  ClusterIP   10.100.200.15   <none>        80/TCP    10m


You can test it inside a pod:

kubectl exec -it <your-pod-name> -- curl http://my-app-service:80


***another way to create cluster ip service for existing deployment.
--------------------------------------------------------------------

* kubectl expose deployment my-deployments --type=ClusterIP --name=my-service --port=80 --target-port=80


What happens:

Type: ClusterIP ‚Üí Default service type. It makes the Service accessible only inside the cluster using the ClusterIP.

--name=my-service ‚Üí Service will be named my-service.

--port=80 ‚Üí Port the Service exposes inside the cluster.

--target-port=80 ‚Üí The port in your Pods that the Service forwards traffic to.

Check service:
*kubectl get svc my-service


You‚Äôll see something like:

NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-service   ClusterIP   10.96.172.34   <none>        80/TCP    5s

Access:

Accessible only from inside the cluster (e.g., from another Pod, or using kubectl port-forward).

Example:

curl http://10.96.172.34:80




if you not install the curl you will install the curl 
Install curl inside your running pod

If your container has a package manager (apt, apk, yum), you can install curl inside:

apt update && apt install -y curl

Then:

curl http://172.20.85.52:8080

                                                                                                  
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------




                                                                                                 5). service object 

                                                                                                    B) node port ip


Creates a Deployment object with usecases

*Uses a Secret for pulling private Docker images
*adding the 2 replicas
*Exposes the app on port 8010 (your Gunicorn app)
*Exposes the app externally using a NodePort Service
*adding the node selector


*touch deployment.yaml
*vim deployment.yaml
*cat deployment.yaml


# ---------------- Deployment ----------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment                       # Name of the deployment
  labels:
    app: my-deployment                       # Label for identifying this deployment
spec:
  replicas: 2                                # Number of pod replicas
  selector:
    matchLabels:
      app: my-deployment                     # Match pods with this label
  template:
    metadata:
      labels:
        app: my-deployment                   # Pods created will have this label
    spec:
      nodeSelector:                          # Ensures pods only schedule on nodes with this label
        vandana: vinay                       # Node must have label vandana=vinay
      containers:
      - name: my-deployment
        image: vinaykumars064/cicd-repo:latest   # Container image (update if needed)
        ports:
        - containerPort: 8010                # Gunicorn listens inside the pod on port 8010
      imagePullSecrets:
      - name: my-docker-secret               # Secret for pulling private images

---
# ---------------- Service ----------------
apiVersion: v1
kind: Service
metadata:
  name: my-service                           # Service name
  labels:
    app: my-service
spec:
  type: NodePort                             # Expose service externally via node‚Äôs IP + NodePort
  selector:
    app: my-deployment                       # Must match pod label, so traffic routes correctly
  ports:
  - port: 80                                 # Service port (clients use this)
    targetPort: 8010                         # Maps to containerPort 8010 in the pod
    nodePort: 30080                          # Fixed NodePort (between 30000‚Äì32767)





‚úÖ Apply this YAML
kubectl apply -f deployment.yaml


‚úÖ Get NodePort and Test
Get service details:

* kubectl get svc <my-app-service>

Example output:

NAME             TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
my-app-service   NodePort   10.100.200.15  <none>        80:30080/TCP   1m

* kubectl get nodes -o wide

Get your EC2 node public IP:

* curl ifconfig.me

Test from your EC2 or browser:

* curl http://<EC2-Public-IP>:30080




***another way to create nodeport ip service for existing deployment.
----------------------------------------------------------------------

** kubectl expose deployment my-deployments --type=NodePort --name=my-service --port=80 --target-port=80


Explanation of flags:

deployment my-deployments ‚Üí Refers to the Deployment you already created.

--type=NodePort ‚Üí Exposes the Service on a port of each Node (between 30000‚Äì32767).

--name=my-service ‚Üí The Service will be named my-service.

--port=80 ‚Üí The port your Service exposes (clients will use this).

--target-port=80 ‚Üí The port inside your Pod containers that traffic gets forwarded to.


Example:

If you run the above, you‚Äôll get a Service. You can check it with:

*kubectl get svc my-service


It will show something like:

NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
my-service   NodePort   10.0.23.45     <none>        80:31543/TCP   5s


Here:

80 ‚Üí Service port

31543 ‚Üí NodePort assigned (random unless you set it with --node-port=31543)

You can then access your app from outside using:

*http://<NodePublicIP>:31543





<<NODE SELECTOR>>
-------------------------

ONE WAY : when we created the node group that time also add the lables 
SECOND WAY : when we created the node group that time whithout adding the lables using below mentioned steaps

üîπ Step 1: Label your EKS Nodes

Pick a node and add a label (example: vandana=vinay):

*kubectl get nodes


Output example:

NAME                                          STATUS   ROLES    AGE   VERSION
ip-10-0-3-12.ap-south-1.compute.internal      Ready    <none>   1h    v1.33.4-eks
ip-10-0-3-210.ap-south-1.compute.internal     Ready    <none>   1h    v1.33.4-eks


Now label them:

*kubectl label node ip-10-0-3-12.ap-south-1.compute.internal vandana=vinay
*kubectl label node ip-10-0-3-210.ap-south-1.compute.internal vandana=vinay

üîπ Step 2: Verify Node Labels

*kubectl get nodes --show-labels | grep vandana


You should see something like:

...,kubernetes.io/os=linux,vinay=vandana

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


                                                                                                   5). service object 

                                                                                                    B) load balancing ip


when we create the subnets that time tag the subnets:
-----------------------------------------------------
‚úÖ Fix

You need to tag your subnets properly in AWS:

**Public Subnets (for external LoadBalancer):
Tag key: kubernetes.io/role/elb
Value: 1

**Private Subnets (for internal LoadBalancer):
Tag key: kubernetes.io/role/internal-elb
Value: 1

**All subnets used by the cluster also need:
Tag key: kubernetes.io/cluster/<your-cluster-name>
Value: owned (if created by EKS) or shared


another way to tag the subnets: You can add via AWS CLI too:
------------------------------------------------------------

aws ec2 create-tags \
  --resources <subnet-id> \
  --tags Key=kubernetes.io/role/internal-elb,Value=1 \
         Key=kubernetes.io/cluster/<your-cluster-name>,Value=shared



then 

*touch deployment.yaml
*vim deployment.yaml


apiVersion: apps/v1
kind: Deployment
metadata:
  name: test123
  labels:
    app: test123
spec:
  replicas: 2
  selector:
    matchLabels:
      app: test123
  template:
    metadata:
      labels:
        app: test123
    spec:
      containers:
        - name: test123
          image: vinaykumars064/cicd-repo:latest
          ports:
            - containerPort: 80
      imagePullSecrets:
        - name: my-secret


*kubectl apply -f deployment.yaml
now app deployment is complited.


then creating the service deployment: in this method service is not dependent for any deployment its saparate its indipendent.
-------------------------------------------------------------------------------------------------------------------------------
touch service.yaml
vim service.yaml


apiVersion: v1
kind: Service
metadata:
  name: test123-svc
  namespace: amma
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: external
spec:
  selector:
    app: test123
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
  type: LoadBalancer



*kubectl apply -f service.yaml
*kubectl get svc
show external ip



***another way to create load balancing ip service for existing deployment.
-----------------------------------------------------------------------------------

*kubectl expose deployment my-deployments --type=LoadBalancer --name=my-service --port=80 --target-port=80


What happens:

Type: LoadBalancer ‚Üí Creates an external cloud load balancer (like AWS ELB, GCP LB, Azure LB) and assigns an external IP or hostname.

--port=80 ‚Üí Clients use this port to connect.

--target-port=80 ‚Üí Traffic gets forwarded to port 80 in the Pod containers.

--name=my-service ‚Üí Service is named my-service.

Check service:
*kubectl get svc my-service


You‚Äôll see something like:

NAME         TYPE           CLUSTER-IP     EXTERNAL-IP       PORT(S)        AGE
my-service   LoadBalancer   10.96.33.120   a1b2c3d4.elb.amazonaws.com   80:31234/TCP   5s


Cluster-IP: Internal service IP

EXTERNAL-IP: Public IP or DNS of your load balancer (may take a few minutes to appear).

80:31234 ‚Üí 80 is service port, 31234 is nodeport backing the LB.

Access:

Once the EXTERNAL-IP is ready, you can test it:

curl http://<EXTERNAL-IP>:80


‚ö†Ô∏è Since you‚Äôre using EKS (AWS) from your earlier messages:

Make sure your worker nodes have IAM policies that allow creating ELBs.

Subnets must be public (with internet gateway) if you need a public external IP.



